{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from uuid import UUID, uuid4\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mypy_boto3_s3.client import S3Client\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from django.core.files.uploadedfile import SimpleUploadedFile\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan, bulk\n",
    "\n",
    "from redbox.models import Settings, Chunk, File\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.model_db import SentenceTransformerDB\n",
    "from redbox.parsing import chunk_file\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "from dj_notebook import activate, Plus\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(ROOT / '.env'))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ENV = Settings(\n",
    "    minio_host=\"localhost\", \n",
    "    object_store=\"minio\", \n",
    "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
    ")\n",
    "MODEL = ENV.embedding_model\n",
    "\n",
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()\n",
    "\n",
    "try:\n",
    "    S3_CLIENT.create_bucket(\n",
    "        Bucket=ENV.bucket_name,\n",
    "        CreateBucketConfiguration={\"LocationConstraint\": ENV.aws_region},\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] != \"BucketAlreadyOwnedByYou\":\n",
    "        raise\n",
    "\n",
    "sys.path.insert(0, str(ROOT / \"django_app\"))\n",
    "\n",
    "RB_APP = activate(\n",
    "    dotenv_file=str(ROOT / \"django_app/.env\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick upload\n",
    "\n",
    "The worker takes forever locally. This notebook will chunk and upload stuff using your local machine, which is much quicker for me. Unlike the eval notebooks, this also makes entried in the Postgres database.\n",
    "\n",
    "This notebook needs the following services running:\n",
    "\n",
    "```\n",
    "docker compose up core-api db -d\n",
    "```\n",
    "\n",
    "It's also important that both `.env` files contain the same embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your user UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RB_APP.read_frame(RB_APP.User.objects.all())[[\"id\", \"email\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_UUID = UUID(\"5c37bf4c-002c-458d-9e68-03042f76a5b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider clearing all files in Elastic and Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RB_APP.read_frame(RB_APP.File.objects.all())[[\"id\", \"original_file_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_index(index: str, es: Elasticsearch) -> None:\n",
    "    documents = scan(es, index=index, query={\"query\": {\"match_all\": {}}})\n",
    "    bulk_data = [\n",
    "        {\"_op_type\": \"delete\", \"_index\": doc['_index'], \"_id\": doc['_id']} for doc in documents\n",
    "    ]\n",
    "    bulk(es, bulk_data, request_timeout=300)\n",
    "\n",
    "def clear_bucket(bucket: str, s3: S3Client) -> None:\n",
    "    response = s3.list_objects_v2(Bucket=bucket)\n",
    "    if \"Contents\" in response:\n",
    "        # Delete each object\n",
    "        for obj in response[\"Contents\"]:\n",
    "            s3.delete_object(Bucket=bucket, Key=obj[\"Key\"])\n",
    "\n",
    "clear_index(index=\"redbox-data-chunk\", es=ES_CLIENT)\n",
    "clear_index(index=\"redbox-data-file\", es=ES_CLIENT)\n",
    "_ = RB_APP.File.objects.all().delete()\n",
    "clear_bucket(bucket=\"redbox-storage-dev\", s3=S3_CLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bucket_objects(bucket: str, s3: S3Client = S3_CLIENT) -> int:\n",
    "    object_count = 0\n",
    "    response = s3.list_objects_v2(Bucket=bucket)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        object_count += len(response['Contents'])\n",
    "        # Paginate\n",
    "        while response['IsTruncated']:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "            response = s3.list_objects_v2(Bucket=bucket, ContinuationToken=continuation_token)\n",
    "            object_count += len(response['Contents'])\n",
    "\n",
    "    return object_count\n",
    "\n",
    "def count_uploads(\n",
    "    es: Elasticsearch = ES_CLIENT, \n",
    "    dj_shell: Plus = RB_APP, \n",
    "    s3: S3Client = S3_CLIENT\n",
    "):\n",
    "    return {\n",
    "        \"django_files\": dj_shell.File.objects.count(),\n",
    "        \"s3_files\": count_bucket_objects(bucket=\"redbox-storage-dev\", s3=s3),\n",
    "        \"elastic_files\": es.count(index=\"redbox-data-file\", body={\"query\": {\"match_all\": {}}})[\"count\"],\n",
    "        \"elastic_chunks\": es.count(index=\"redbox-data-chunk\", body={\"query\": {\"match_all\": {}}})[\"count\"],\n",
    "    }\n",
    "\n",
    "count_uploads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we embed and upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_django(\n",
    "    file_path: Path,\n",
    "    user_uuid: UUID = USER_UUID,\n",
    "    dj_shell: Plus = RB_APP,\n",
    "):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        file = dj_shell.File.objects.create(\n",
    "            status=\"complete\",\n",
    "            original_file=SimpleUploadedFile(\n",
    "                name=file_path.name,\n",
    "                content=f.read()\n",
    "            ),\n",
    "            user=RB_APP.User.objects.get(id=user_uuid),\n",
    "            original_file_name=file_path.name,\n",
    "            core_file_uuid=uuid4(),\n",
    "        )\n",
    "        file.save()\n",
    "    \n",
    "    return file\n",
    "\n",
    "def embed_and_upload_file(\n",
    "    file_path: Path,\n",
    "    model: Embeddings,\n",
    "    user_uuid: UUID = USER_UUID,\n",
    "    es_client: Elasticsearch = ES_CLIENT,\n",
    "    s3_client: S3Client = S3_CLIENT,\n",
    "    dj_shell: Plus = RB_APP,\n",
    ") -> None:\n",
    "    print(f\"Processing {file_path.name}\")\n",
    "          \n",
    "    # Add to Django\n",
    "    dj_file = add_to_django(file_path=file_path, user_uuid=user_uuid, dj_shell=dj_shell)\n",
    "\n",
    "    es_file = File(\n",
    "        uuid=dj_file.core_file_uuid,\n",
    "        key=dj_file.url.parts[-1], \n",
    "        bucket=dj_file.url.parts[1], \n",
    "        creator_user_uuid=user_uuid,\n",
    "    )\n",
    "\n",
    "    # Add to S3\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, \"redbox-storage-dev\", dj_file.url.parts[-1])\n",
    "\n",
    "    print(f\"Added {file_path.name} to S3 and Django\")\n",
    "\n",
    "    # Chunk\n",
    "    chunks = chunk_file(file=es_file, s3_client=s3_client)\n",
    "\n",
    "    print(f\"Chunked {file_path.name} ({len(chunks)} chunks)\")\n",
    "\n",
    "    # Embed\n",
    "    embeddings = [embedding.embedding for embedding in model.embed_sentences([chunk.text for chunk in chunks]).data]\n",
    "\n",
    "    print(f\"Embedded {file_path.name} ({len(embeddings)} chunks)\")\n",
    "\n",
    "    # Merge\n",
    "    es_chunks = []\n",
    "    for chunk, embedding in zip(chunks, embeddings, strict=True):\n",
    "        chunk_embedded = Chunk(\n",
    "            uuid=chunk.uuid,\n",
    "            created_datetime=chunk.created_datetime,\n",
    "            creator_user_uuid=chunk.creator_user_uuid,\n",
    "            parent_file_uuid=chunk.parent_file_uuid,\n",
    "            index=chunk.index,\n",
    "            text=chunk.text,\n",
    "            metadata=chunk.metadata,\n",
    "            embedding=embedding,\n",
    "        )\n",
    "        es_chunks.append(chunk_embedded)\n",
    "\n",
    "    # Add to Elastic\n",
    "    es_client.index(\n",
    "        index=\"redbox-data-file\",\n",
    "        id=es_file.uuid,\n",
    "        body=es_file.model_dump_json(),\n",
    "    )\n",
    "    \n",
    "    for chunk in es_chunks:\n",
    "        es_client.index(\n",
    "            index=\"redbox-data-chunk\",\n",
    "            id=chunk.uuid,\n",
    "            body=chunk.model_dump_json(),\n",
    "        )\n",
    "\n",
    "    print(f\"{file_path.name} complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/Demo Data\")\n",
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/D&D\")\n",
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/Lit\")\n",
    "DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/DS\")\n",
    "\n",
    "for file_path in DIR_TO_UPLOAD.rglob(\"[!.]*.*\"):\n",
    "    embed_and_upload_file(\n",
    "        file_path=file_path,\n",
    "        model=SentenceTransformerDB(embedding_model_name=MODEL),\n",
    "        user_uuid=USER_UUID,\n",
    "        es_client=ES_CLIENT,\n",
    "        s3_client=S3_CLIENT,\n",
    "        dj_shell=RB_APP,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it uploaded okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_uploads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can bring up the Django app and use the files.\n",
    "\n",
    "```\n",
    "docker compose up django-app -d --wait\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
