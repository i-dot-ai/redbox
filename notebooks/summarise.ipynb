{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/Users/willlangdale/Library/Caches/pypoetry/virtualenvs/redbox-Vh_-Fb0j-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from http import HTTPStatus\n",
    "from typing import Annotated\n",
    "from uuid import UUID\n",
    "\n",
    "from fastapi import Depends, FastAPI, HTTPException, WebSocket\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from core_api.src.auth import get_user_uuid\n",
    "from redbox.llm.prompts.chat import (\n",
    "    CONDENSE_QUESTION_PROMPT,\n",
    "    STUFF_DOCUMENT_PROMPT,\n",
    "    WITH_SOURCES_PROMPT,\n",
    ")\n",
    "from redbox.model_db import MODEL_PATH\n",
    "from redbox.models import EmbeddingModelInfo, Settings\n",
    "from redbox.models.chat import ChatRequest, ChatResponse, SourceDocument\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "\n",
    "env = Settings(_env_file=\"../.env\")\n",
    "env.elastic.host = \"localhost\"\n",
    "env.minio_host = \"localhost\"\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=env.embedding_model, cache_folder=\"../models/\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": env.elastic.port,\n",
    "            \"scheme\": env.elastic.scheme,\n",
    "        }\n",
    "    ],\n",
    "    basic_auth=(env.elastic.user, env.elastic.password),\n",
    ")\n",
    "\n",
    "if env.elastic.subscription_level == \"basic\":\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
    "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=\"redbox-data-chunk\",\n",
    "    embedding=embedding_model,\n",
    "    strategy=strategy,\n",
    "    vector_query_field=\"embedding\",\n",
    ")\n",
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "\n",
    "llm = ChatLiteLLM(\n",
    "    model=env.azure_openai_model,\n",
    "    streaming=True,\n",
    "    azure_key=env.azure_openai_api_key,\n",
    "    api_version=env.openai_api_version,\n",
    "    api_base=env.azure_openai_endpoint,\n",
    "    max_tokens=4_096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarisation scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from redbox.models.file import Metadata\n",
    "from functools import reduce\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.schema import StrOutputParser\n",
    "from redbox.llm.prompts.core import _core_redbox_prompt\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/redbox-data-chunk/_search?scroll=5m [status:200 duration:0.009s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/_search/scroll [status:200 duration:0.003s]\n",
      "INFO:elastic_transport.transport:DELETE http://localhost:9200/_search/scroll [status:200 duration:0.002s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/redbox-data-chunk/_search?scroll=5m [status:200 duration:0.006s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/_search/scroll [status:200 duration:0.002s]\n",
      "INFO:elastic_transport.transport:DELETE http://localhost:9200/_search/scroll [status:200 duration:0.001s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m  \u001b[38;5;66;03m# parameterise later\u001b[39;00m\n\u001b[1;32m     33\u001b[0m doc_token_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum([doc\u001b[38;5;241m.\u001b[39mtoken_count \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents])\n\u001b[0;32m---> 34\u001b[0m doc_token_sum_limit_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc_token_sum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m doc_token_sum_limit_index\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# documents_trunc = documents[:doc_token_sum_limit_index]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# if len(documents) < doc_token_sum_limit_index:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     print(\"Documents were longer than 20k tokens. Truncating to the first 20k.\")\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "from core_api.src.runnables import make_es_retriever, make_stuff_document_runnable\n",
    "from core_api.src.format import get_file_chunked_to_tokens\n",
    "from redbox.models import ChatRequest, Chunk\n",
    "import numpy as np\n",
    "\n",
    "chat_request = ChatRequest(\n",
    "    **{\n",
    "        \"message_history\": [\n",
    "            {\n",
    "                \"text\": \"Please summarise the contents of the uploaded files, what are the key energy stats\", \"role\": \"user\"\n",
    "            },\n",
    "        ],\n",
    "        \"selected_files\": [ \n",
    "            {\"uuid\": \"718dfb9c-3f0c-4942-a0c1-e0458a7a53c6\"},\n",
    "            {\"uuid\": \"a28c04e2-8a1c-41b0-8d29-74ae41aa2e0f\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = make_stuff_document_runnable(system_prompt=\"Summarise stuff\", llm=llm)\n",
    "\n",
    "documents: list[Chunk] = []\n",
    "for selected_file in chat_request.selected_files:\n",
    "    chunks = get_file_chunked_to_tokens(\n",
    "        file_uuid=selected_file.uuid,\n",
    "        user_uuid=UUID(\"b92ebddb-a77e-4ed7-81b9-a2f7ce814ef5\"),\n",
    "        storage_handler=storage_handler,\n",
    "    )\n",
    "    documents += chunks\n",
    "\n",
    "# right now, can only handle a single document so we manually truncate\n",
    "max_tokens = 5000  # parameterise later\n",
    "doc_token_sum = np.cumsum([doc.token_count for doc in documents])\n",
    "doc_token_sum_limit_index = len([i for i in doc_token_sum if i < max_tokens])\n",
    "doc_token_sum_limit_index\n",
    "\n",
    "# documents_trunc = documents[:doc_token_sum_limit_index]\n",
    "# if len(documents) < doc_token_sum_limit_index:\n",
    "#     print(\"Documents were longer than 20k tokens. Truncating to the first 20k.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "\n",
    "def get_file_as_documents(\n",
    "    file_uuid: UUID,\n",
    "    user_uuid: UUID,\n",
    "    storage_handler: ElasticsearchStorageHandler = storage_handler,\n",
    "    max_tokens: int | None = None\n",
    ") -> list[Document]:\n",
    "    \"\"\"Gets a file as LangChain Documents, splitting it by max_tokens.\"\"\"\n",
    "    documents: list[Document] = []\n",
    "    chunks_unsorted = storage_handler.get_file_chunks(parent_file_uuid=file_uuid, user_uuid=user_uuid)\n",
    "    chunks = sorted(chunks_unsorted, key=lambda x: x.index)\n",
    "\n",
    "    total_tokens = sum(chunk.token_count for chunk in chunks)\n",
    "    print(total_tokens)\n",
    "\n",
    "    token_count: int = 0\n",
    "    n = max_tokens or float(\"inf\")\n",
    "    page_content: list[str] = []\n",
    "    metadata: list[Metadata | None] = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if token_count + chunk.token_count >= n:\n",
    "            document = Document(\n",
    "                page_content=\" \".join(page_content),\n",
    "                metadata=reduce(Metadata.merge, metadata),\n",
    "            )\n",
    "            documents.append(document)\n",
    "            token_count = 0\n",
    "            page_content = []\n",
    "            metadata = []\n",
    "\n",
    "        page_content.append(chunk.text)\n",
    "        metadata.append(chunk.metadata)\n",
    "        token_count += chunk.token_count\n",
    "\n",
    "    if len(page_content) > 0:\n",
    "        document = Document(\n",
    "            page_content=\" \".join(page_content),\n",
    "            metadata=reduce(Metadata.merge, metadata),\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(\n",
    "    chat_request: ChatRequest,\n",
    "    file_uuid: UUID,\n",
    "    user_uuid: UUID,\n",
    "    llm: ChatLiteLLM,\n",
    "    storage_handler: ElasticsearchStorageHandler,\n",
    ") -> ChatResponse:\n",
    "    question = chat_request.message_history[-1].text\n",
    "    previous_history = list(chat_request.message_history[:-1])\n",
    "    \n",
    "    # get full doc from vector store\n",
    "    documents = get_file_as_documents(\n",
    "        file_uuid=file_uuid, \n",
    "        user_uuid=user_uuid, \n",
    "        storage_handler=storage_handler,\n",
    "        max_tokens=20_000\n",
    "    )\n",
    "    \n",
    "    # right now, can only handle a single document so we manually truncate\n",
    "    document = documents[:1]\n",
    "    if len(documents) > 1:\n",
    "        print(\"Document was longer than 20k tokens. Truncating to the first 20k.\")\n",
    "    \n",
    "    # stuff raw prompt\n",
    "    chat_history = [\n",
    "        (\"system\", _core_redbox_prompt),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "        (\"user\", \"Question: {question}. \\n\\n Content: \\n\\n<document> {content} </document> \\n\\n Answer: \"),\n",
    "    ]\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"messages\": itemgetter(\"messages\"),\n",
    "            \"content\": itemgetter(\"content\") | RunnableLambda(format_docs),\n",
    "        }\n",
    "        | ChatPromptTemplate.from_messages(chat_history)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # return\n",
    "    return chain.invoke(\n",
    "        input={\n",
    "            \"question\": question,\n",
    "            \"content\": document,\n",
    "            \"messages\": [(msg.role, msg.text) for msg in previous_history]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "chat_request_body = {\n",
    "    \"message_history\": [\n",
    "        # {\"text\": \"Can you always refer to BEIS as the Department for Business, Energy and Industrial Strategy from now on?\", \"role\": \"user\"},\n",
    "        # {\"text\": \"Of course. In future responses I will always expand the BEIS acronym.\", \"role\": \"ai\"},\n",
    "        {\"text\": \"Please summarise all the key people in this document and who they work for.\", \"role\": \"user\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = summarise(\n",
    "    chat_request=ChatRequest(**chat_request_body),\n",
    "    file_uuid=UUID(\"35b3d95f-7f65-4cae-b159-22001ca19c88\"),\n",
    "    user_uuid=UUID(\"b92ebddb-a77e-4ed7-81b9-a2f7ce814ef5\"),\n",
    "    llm=llm,\n",
    "    storage_handler=storage_handler\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://localhost:9200/redbox-data-chunk/_search?scroll=5m [status:200 duration:0.089s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/_search/scroll [status:200 duration:0.102s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/_search/scroll [status:200 duration:0.008s]\n",
      "INFO:elastic_transport.transport:POST http://localhost:9200/_search/scroll [status:200 duration:0.003s]\n",
      "INFO:elastic_transport.transport:DELETE http://localhost:9200/_search/scroll [status:200 duration:0.002s]\n"
     ]
    }
   ],
   "source": [
    "from redbox.models.file import Metadata, Chunk\n",
    "from functools import partial, reduce\n",
    "from uuid import UUID\n",
    "\n",
    "chunks_unsorted = storage_handler.get_file_chunks(\n",
    "    parent_file_uuid=UUID(\"35b3d95f-7f65-4cae-b159-22001ca19c88\"), \n",
    "    user_uuid=UUID(\"b92ebddb-a77e-4ed7-81b9-a2f7ce814ef5\")\n",
    ")\n",
    "chunks = sorted(chunks_unsorted, key=lambda x: x.index)\n",
    "\n",
    "def reduce_chunks_by_tokens(chunks: list[Chunk] | None, chunk: Chunk, max_tokens: int) -> list[Chunk]:\n",
    "    \"\"\"\"\"\"\n",
    "    if not chunks:\n",
    "        return [chunk]\n",
    "    \n",
    "    last_chunk = chunks[-1]\n",
    "\n",
    "    if chunk.token_count + last_chunk.token_count <= max_tokens:\n",
    "        chunks[-1] = Chunk(\n",
    "            parent_file_uuid=last_chunk.parent_file_uuid,\n",
    "            index=last_chunk.index,\n",
    "            text=last_chunk.text + chunk.text,\n",
    "            metadata=Metadata.merge(last_chunk.metadata, chunk.metadata),\n",
    "            creator_user_uuid=last_chunk.creator_user_uuid,\n",
    "        )\n",
    "    else:\n",
    "        chunk.index = last_chunk.index + 1\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "reduce_chunk_t300 = partial(reduce_chunks_by_tokens, max_tokens=300)\n",
    "\n",
    "result = reduce(lambda cs, c: reduce_chunk_t300(cs, c), chunks, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2066, 301, 1, 84.15295256534365)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(chunks), max(chunk.token_count for chunk in chunks), min(chunk.token_count for chunk in chunks), np.mean([chunk.token_count for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685, 301, 111, 253.76058394160583)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(result), max(chunk.token_count for chunk in result), min(chunk.token_count for chunk in result), np.mean([chunk.token_count for chunk in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chunk.index for chunk in result]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
