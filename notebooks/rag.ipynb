{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from uuid import UUID\n",
                "from pathlib import Path\n",
                "import tiktoken\n",
                "import os\n",
                "import logging\n",
                "import sys\n",
                "\n",
                "from langchain_community.chat_models import ChatLiteLLM\n",
                "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
                "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
                "from elasticsearch import Elasticsearch\n",
                "\n",
                "from redbox.models import Settings\n",
                "from redbox.models.settings import ElasticLocalSettings\n",
                "from redbox.storage import ElasticsearchStorageHandler\n",
                "\n",
                "from core_api.callbacks import LoggerCallbackHandler\n",
                "\n",
                "from dotenv import find_dotenv, load_dotenv\n",
                "\n",
                "ROOT = Path().resolve().parent\n",
                "\n",
                "_ = load_dotenv(find_dotenv(ROOT / '.env'))\n",
                "\n",
                "logging.basicConfig(steam=sys.stdout, level=logging.INFO)\n",
                "log = logging.getLogger()\n",
                "\n",
                "env = Settings(\n",
                "    _env_file=(ROOT / '.env'),\n",
                "    minio_host=\"localhost\", \n",
                "    object_store=\"minio\",\n",
                "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
                ")\n",
                "\n",
                "embedding_model = SentenceTransformerEmbeddings(model_name=env.embedding_model, cache_folder=\"../models/\")\n",
                "\n",
                "es = Elasticsearch(\n",
                "    hosts=[\n",
                "        {\n",
                "            \"host\": \"localhost\",\n",
                "            \"port\": env.elastic.port,\n",
                "            \"scheme\": env.elastic.scheme,\n",
                "        }\n",
                "    ],\n",
                "    basic_auth=(env.elastic.user, env.elastic.password),\n",
                ")\n",
                "\n",
                "if env.elastic.subscription_level == \"basic\":\n",
                "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
                "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
                "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
                "\n",
                "vector_store = ElasticsearchStore(\n",
                "    es_connection=es,\n",
                "    index_name=\"redbox-data-chunk\",\n",
                "    embedding=embedding_model,\n",
                "    strategy=strategy,\n",
                "    vector_query_field=\"embedding\",\n",
                ")\n",
                "\n",
                "# See core_api.dependecies for details on this hack\n",
                "os.environ[\"AZURE_API_VERSION\"] = env.openai_api_version\n",
                "\n",
                "logger_callback = LoggerCallbackHandler(logger=log)\n",
                "\n",
                "llm = ChatLiteLLM(\n",
                "    model=env.azure_openai_model,\n",
                "    streaming=True,\n",
                "    azure_key=env.azure_openai_api_key,\n",
                "    api_base=env.azure_openai_endpoint,\n",
                "    max_tokens=1_024,\n",
                "    callbacks=[logger_callback]\n",
                ")\n",
                "\n",
                "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
                "\n",
                "tokeniser = tiktoken.get_encoding(\"cl100k_base\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from core_api.retriever import ParameterisedElasticsearchRetriever\n",
                "from langchain_core.runnables import ConfigurableField\n",
                "\n",
                "def get_parameterised_retriever(\n",
                "    env, \n",
                "    es\n",
                "):\n",
                "    \"\"\"Creates an Elasticsearch retriever runnable.\n",
                "\n",
                "    Runnable takes input of a dict keyed to question, file_uuids and user_uuid.\n",
                "\n",
                "    Runnable returns a list of Chunks.\n",
                "    \"\"\"\n",
                "    default_params = {\n",
                "        \"size\": env.ai.rag_k,\n",
                "        \"num_candidates\": env.ai.rag_num_candidates,\n",
                "        \"match_boost\": 1,\n",
                "        \"knn_boost\": 1,\n",
                "        \"similarity_threshold\": 0,\n",
                "    }\n",
                "    return ParameterisedElasticsearchRetriever(\n",
                "        es_client=es,\n",
                "        index_name=f\"{env.elastic_root_index}-chunk\",\n",
                "        params=default_params,\n",
                "        embedding_model=embedding_model,\n",
                "    ).configurable_fields(\n",
                "        params=ConfigurableField(\n",
                "            id=\"params\", name=\"Retriever parameters\", description=\"A dictionary of parameters to use for the retriever.\"\n",
                "        )\n",
                "    )\n",
                "\n",
                "retriever = get_parameterised_retriever(env, es)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "retriever.invoke(\n",
                "    input={\n",
                "        \"question\": \"KAN\",\n",
                "        \"file_uuids\": [\n",
                "            \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
                "            # \"1a9d18a7-9499-47b6-abcc-4e82370028ee\" # MAMBA paper\n",
                "        ],\n",
                "        \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\"\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:elastic_transport.transport:POST http://localhost:9200/redbox-data-chunk/_search [status:200 duration:0.018s]\n",
                        "INFO:root:LLM start: {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'repr': \"ChatLiteLLM(callbacks=[<core_api.callbacks.LoggerCallbackHandler object at 0x151a670d0>], client=<module 'litellm' from '/Users/willlangdale/Library/Caches/pypoetry/virtualenvs/redbox-Vh_-Fb0j-py3.11/lib/python3.11/site-packages/litellm/__init__.py'>, model='azure/gpt-4', openai_api_key='', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', streaming=True, api_base='https://oai-i-dot-ai-playground-sweden.openai.azure.com/', max_tokens=1024, huggingface_api_key='', together_ai_api_key='')\", 'name': 'ChatLiteLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatLiteLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'name': 'ChatLiteLLM'}}, {'id': 2, 'type': 'schema', 'data': 'ChatLiteLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, prompts: [\"System: Given the following conversation and extracted parts of a long document and a question, create a final answer. \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer. If a user asks for a particular format to be returned, such as bullet points, then please use that format. If a user asks for bullet points you MUST give bullet points. If the user asks for a specific number or range of bullet points you MUST give that number of bullet points. \\nUse **bold** to highlight the most question relevant parts in your response. If dealing dealing with lots of data return it in markdown table format. \\nHuman: What is the fastest attention that the authors are aware of?\\nAI: The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\\nHuman: Give the full citation. \\n=========\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state ℎ. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n 4.1 Synthetic Tasks Full experiment details for these tasks including task details and training protocol are in Appendix E.1. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n incorporates positional information to emphasize locality. Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n We give several models an “improved recipe”, inspired by changes adopted by popular large language models such as PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\\n\\nlinear learning rate warmup with cosine decay to 1𝑒 − 5, with a peak value of 5× the GPT3 value \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n the sequence length grows. (We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic computation \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n Table 2: (Induction Heads.) Models are trained on sequence length 28 = 256, and tested on increasing sequence lengths of 26 = 64 up to 220 = 1048576. Full numbers in Table 11.\\n\\n\\x00\\x15\\x00\\x14\\x00\\x15\\n\\n\\x00\\x16\\x00\\x82\\x00\\x15\\x00\\x14\\x00\\x15\\x004\\x00I\\x00V\\x00T\\x00P\\x00I\\x00\\\\\\x00M\\x00X\\x00]\\x00\\x04\\x00\\x0c\\x00P\\x00S\\x00K\\x00\\x04\\x00W\\x00G\\x00E\\x00P\\x00I\\x00\\n\\n\\x008\\x00V\\x00E\\x00R\\x00W\\x00J\\x00S\\x00V\\x00Q\\x00I\\x00V\\n\\n\\x00\\x15\\x00\\x14\\x00\\x16\\x00\\x14 \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n in the GAAM case). \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n similar, contrary to the findings in Poli et al. (2023). \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019). \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n Models are causal and therefore only the last element (across the sequence length) of the model’s output is used for the classification head. Note that we control for the total number of elements in the loss function per gradient step. The pretraining objective includes all positions across the \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n discussion of the dependence of the constant on the dimension as a future work. \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The performance differences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should be LTI, but once they are “tokenized” and compressed by the \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n hyperparameter balancing the effect of the two terms. \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n=========\\nFINAL ANSWER: \"]\n",
                        "INFO:httpx:HTTP Request: POST https://oai-i-dot-ai-playground-sweden.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
                        "INFO:root:LLM end: generations=[[ChatGeneration(text=\"I'm sorry, but I can't provide the full citation as it's not given in the provided document excerpts.\", message=AIMessage(content=\"I'm sorry, but I can't provide the full citation as it's not given in the provided document excerpts.\", id='run-2263eeba-8e71-41fc-aee5-860ab1587e32-0'))]] llm_output=None run=None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'response': \"I'm sorry, but I can't provide the full citation as it's not given in the provided document excerpts.\",\n",
                            " 'source_documents': [Document(page_content='Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 120, 'page_number': 7, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.885216+00:00', 'token_count': 29}),\n",
                            "  Document(page_content='The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state ℎ.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 112, 'page_number': 7, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.884697+00:00', 'token_count': 42}),\n",
                            "  Document(page_content='4.1 Synthetic Tasks Full experiment details for these tasks including task details and training protocol are in Appendix E.1.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 169, 'page_number': 10, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.889010+00:00', 'token_count': 24}),\n",
                            "  Document(page_content='incorporates positional information to emphasize locality. Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 496, 'page_number': 27, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.912148+00:00', 'token_count': 59}),\n",
                            "  Document(page_content='We give several models an “improved recipe”, inspired by changes adopted by popular large language models such as PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\\n\\nlinear learning rate warmup with cosine decay to 1𝑒 − 5, with a peak value of 5× the GPT3 value', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 547, 'page_number': 30, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.915244+00:00', 'token_count': 84}),\n",
                            "  Document(page_content='the sequence length grows. (We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic computation', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 202, 'page_number': 11, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.891657+00:00', 'token_count': 58}),\n",
                            "  Document(page_content='Table 2: (Induction Heads.) Models are trained on sequence length 28 = 256, and tested on increasing sequence lengths of 26 = 64 up to 220 = 1048576. Full numbers in Table 11.\\n\\n\\x00\\x15\\x00\\x14\\x00\\x15\\n\\n\\x00\\x16\\x00\\x82\\x00\\x15\\x00\\x14\\x00\\x15\\x004\\x00I\\x00V\\x00T\\x00P\\x00I\\x00\\\\\\x00M\\x00X\\x00]\\x00\\x04\\x00\\x0c\\x00P\\x00S\\x00K\\x00\\x04\\x00W\\x00G\\x00E\\x00P\\x00I\\x00\\n\\n\\x008\\x00V\\x00E\\x00R\\x00W\\x00J\\x00S\\x00V\\x00Q\\x00I\\x00V\\n\\n\\x00\\x15\\x00\\x14\\x00\\x16\\x00\\x14', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 189, 'page_number': 11, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.890624+00:00', 'token_count': 143}),\n",
                            "  Document(page_content='in the GAAM case).', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 456, 'page_number': 30, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:57.067709+00:00', 'token_count': 6}),\n",
                            "  Document(page_content='similar, contrary to the findings in Poli et al. (2023).', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 195, 'page_number': 11, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.891057+00:00', 'token_count': 16}),\n",
                            "  Document(page_content='perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 490, 'page_number': 26, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.911763+00:00', 'token_count': 24}),\n",
                            "  Document(page_content='Models are causal and therefore only the last element (across the sequence length) of the model’s output is used for the classification head. Note that we control for the total number of elements in the loss function per gradient step. The pretraining objective includes all positions across the', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 602, 'page_number': 33, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.918699+00:00', 'token_count': 55}),\n",
                            "  Document(page_content='to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 470, 'page_number': 25, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.910471+00:00', 'token_count': 35}),\n",
                            "  Document(page_content='discussion of the dependence of the constant on the dimension as a future work.', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 99, 'page_number': 8, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:56.991335+00:00', 'token_count': 15}),\n",
                            "  Document(page_content='However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The performance differences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should be LTI, but once they are “tokenized” and compressed by the', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 633, 'page_number': 35, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.920298+00:00', 'token_count': 64}),\n",
                            "  Document(page_content='hyperparameter balancing the effect of the two terms.', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 287, 'page_number': 18, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:57.046881+00:00', 'token_count': 10})],\n",
                            " 'route_name': 'search'}"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_core.runnables import (\n",
                "    Runnable,\n",
                "    RunnableLambda,\n",
                "    RunnablePassthrough,\n",
                "    chain,\n",
                ")\n",
                "from langchain.schema import StrOutputParser\n",
                "from operator import itemgetter\n",
                "from redbox.models import ChatRoute\n",
                "from redbox.models.chain import ChainInput\n",
                "\n",
                "from core_api.format import format_documents\n",
                "from core_api.runnables import make_chat_prompt_from_messages_runnable\n",
                "\n",
                "\n",
                "def build_retrieval_chain(\n",
                "    llm,\n",
                "    retriever,\n",
                "    tokeniser,\n",
                "    env,\n",
                ") -> Runnable:\n",
                "    return (\n",
                "        RunnablePassthrough.assign(documents=retriever)\n",
                "        | RunnablePassthrough.assign(\n",
                "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
                "        )\n",
                "        | {\n",
                "            \"response\": make_chat_prompt_from_messages_runnable(\n",
                "                system_prompt=env.ai.retrieval_system_prompt,\n",
                "                question_prompt=env.ai.retrieval_question_prompt,\n",
                "                input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
                "                tokeniser=tokeniser,\n",
                "            )\n",
                "            | llm\n",
                "            | StrOutputParser(),\n",
                "            \"source_documents\": itemgetter(\"documents\"),\n",
                "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
                "        }\n",
                "    )\n",
                "\n",
                "rag = build_retrieval_chain(llm, retriever, tokeniser, env)\n",
                "\n",
                "params = ChainInput(\n",
                "    question=\"Give the full citation.\",\n",
                "    file_uuids=[\n",
                "        \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
                "        \"1a9d18a7-9499-47b6-abcc-4e82370028ee\" # MAMBA paper\n",
                "    ],\n",
                "    user_uuid=\"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
                "    chat_history=[\n",
                "        {\"text\": \"What is the fastest attention that the authors are aware of?\", \"role\": \"user\"},\n",
                "        {\"text\": \"The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\", \"role\": \"ai\"},\n",
                "    ],\n",
                ")\n",
                "\n",
                "rag.invoke(params.model_dump())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:root:LLM start: {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'repr': \"ChatLiteLLM(callbacks=[<core_api.callbacks.LoggerCallbackHandler object at 0x151a670d0>], client=<module 'litellm' from '/Users/willlangdale/Library/Caches/pypoetry/virtualenvs/redbox-Vh_-Fb0j-py3.11/lib/python3.11/site-packages/litellm/__init__.py'>, model='azure/gpt-4', openai_api_key='', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', streaming=True, api_base='https://oai-i-dot-ai-playground-sweden.openai.azure.com/', max_tokens=1024, huggingface_api_key='', together_ai_api_key='')\", 'name': 'ChatLiteLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatLiteLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'name': 'ChatLiteLLM'}}, {'id': 2, 'type': 'schema', 'data': 'ChatLiteLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, prompts: [\"System: Given the following conversation and a follow up question, generate a follow up question to be a standalone question. You are only allowed to generate one question in response. Include sources from the chat history in the standalone question created, when they are available. If you don't know the answer, just say that you don't know, don't try to make up an answer. \\n\\nHuman: What is the fastest attention that the authors are aware of?\\nAI: The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\\nHuman: What is the fastest attention that the authors are aware of?\\n=========\\n Standalone question: \"]\n",
                        "INFO:httpx:HTTP Request: POST https://oai-i-dot-ai-playground-sweden.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
                        "INFO:root:LLM end: generations=[[ChatGeneration(text=\"According to the author's knowledge, which version of FlashAttention is considered the fastest implementation of attention?\", message=AIMessage(content=\"According to the author's knowledge, which version of FlashAttention is considered the fastest implementation of attention?\", id='run-5b25fb65-c689-41b6-a6f9-dd755590fabd-0'))]] llm_output=None run=None\n",
                        "INFO:elastic_transport.transport:POST http://localhost:9200/redbox-data-chunk/_search [status:200 duration:0.030s]\n",
                        "INFO:root:LLM start: {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'repr': \"ChatLiteLLM(callbacks=[<core_api.callbacks.LoggerCallbackHandler object at 0x151a670d0>], client=<module 'litellm' from '/Users/willlangdale/Library/Caches/pypoetry/virtualenvs/redbox-Vh_-Fb0j-py3.11/lib/python3.11/site-packages/litellm/__init__.py'>, model='azure/gpt-4', openai_api_key='', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', streaming=True, api_base='https://oai-i-dot-ai-playground-sweden.openai.azure.com/', max_tokens=1024, huggingface_api_key='', together_ai_api_key='')\", 'name': 'ChatLiteLLM', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatLiteLLMInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_community', 'chat_models', 'litellm', 'ChatLiteLLM'], 'name': 'ChatLiteLLM'}}, {'id': 2, 'type': 'schema', 'data': 'ChatLiteLLMOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, prompts: [\"System: Given the following conversation and extracted parts of a long document and a question, create a final answer. \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer. If a user asks for a particular format to be returned, such as bullet points, then please use that format. If a user asks for bullet points you MUST give bullet points. If the user asks for a specific number or range of bullet points you MUST give that number of bullet points. \\nUse **bold** to highlight the most question relevant parts in your response. If dealing dealing with lots of data return it in markdown table format. \\nHuman: What is the fastest attention that the authors are aware of?\\nAI: The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\\nHuman: According to the author's knowledge, which version of FlashAttention is considered the fastest implementation of attention? \\n=========\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7× faster than without causal mask, since approximately only half of the attention entries are computed. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n memory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5, showing that it is up to 7× times faster than attention at sequence length 32K, and is as memory-efficient as the best attention implementation (FlashAttention). \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n two-layer KANs. To the best of our knowledge, there is not yet a “generalized” version of the theorem that corresponds to deeper KANs. \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n 4.5 Speed and Memory Benchmarks We benchmark the speed of the SSM scan operation (state expansion 𝑁 = 16), as well as the end-to-end inference throughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024)) \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n most memory-efficient Transformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation, and we expect further \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n at the same time. In terms of interpretability, we scale the transparency of each activation according to its magnitude, so it becomes immediately clear which input variables are important without the need for feature attribution (see Figure 4.3 left): signature is mostly dependent on µr, and \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n Out of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly better than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n log λ = 0 which is equivalent to V = µrλ, which is true by definition. It is, however, reassuring that we discover this relation without any prior knowledge. \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n (2) Initialization scales. Each activation function is initialized to have ws = 1 and spline(x) ≈ 0 2. wb is initialized according to the Xavier initialization, which has been used to initialize linear layers in MLPs. \\n</Doc36ed2f1a-57a5-489c-a4cb-fbdd25e2b038>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n (since 𝑫 depends on the data 𝑥). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n [19] Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. In: The International\\n\\nConference on Learning Representations (ICLR). 2024. \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n\\n<Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to \\n</Doc1a9d18a7-9499-47b6-abcc-4e82370028ee>\\n=========\\nFINAL ANSWER: \"]\n",
                        "INFO:httpx:HTTP Request: POST https://oai-i-dot-ai-playground-sweden.openai.azure.com//openai/deployments/gpt-4/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n",
                        "INFO:root:LLM end: generations=[[ChatGeneration(text=\"The speediest implementation of attention as per the authors' knowledge is **FlashAttention-2 (Dao 2024) with a causal mask**. It is reported that it's around **1.7× faster** in comparison to the version without a causal mask.\", message=AIMessage(content=\"The speediest implementation of attention as per the authors' knowledge is **FlashAttention-2 (Dao 2024) with a causal mask**. It is reported that it's around **1.7× faster** in comparison to the version without a causal mask.\", id='run-35fefdae-34b6-4aad-bd74-aed7951328d0-0'))]] llm_output=None run=None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'response': \"The speediest implementation of attention as per the authors' knowledge is **FlashAttention-2 (Dao 2024) with a causal mask**. It is reported that it's around **1.7× faster** in comparison to the version without a causal mask.\",\n",
                            " 'source_documents': [Document(page_content='For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7× faster than without causal mask, since approximately only half of the attention entries are computed.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 647, 'page_number': 36, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.921093+00:00', 'token_count': 62}),\n",
                            "  Document(page_content='memory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5, showing that it is up to 7× times faster than attention at sequence length 32K, and is as memory-efficient as the best attention implementation (FlashAttention).', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 513, 'page_number': 28, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.913120+00:00', 'token_count': 57}),\n",
                            "  Document(page_content='two-layer KANs. To the best of our knowledge, there is not yet a “generalized” version of the theorem that corresponds to deeper KANs.', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 54, 'page_number': 5, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:56.983382+00:00', 'token_count': 34}),\n",
                            "  Document(page_content='4.5 Speed and Memory Benchmarks We benchmark the speed of the SSM scan operation (state expansion 𝑁 = 16), as well as the end-to-end inference throughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024))', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 268, 'page_number': 15, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.896237+00:00', 'token_count': 73}),\n",
                            "  Document(page_content='H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 472, 'page_number': 25, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.910557+00:00', 'token_count': 64}),\n",
                            "  Document(page_content='optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 531, 'page_number': 29, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.914278+00:00', 'token_count': 58}),\n",
                            "  Document(page_content='HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 119, 'page_number': 7, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.885173+00:00', 'token_count': 29}),\n",
                            "  Document(page_content='most memory-efficient Transformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation, and we expect further', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 654, 'page_number': 36, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.921597+00:00', 'token_count': 50}),\n",
                            "  Document(page_content='at the same time. In terms of interpretability, we scale the transparency of each activation according to its magnitude, so it becomes immediately clear which input variables are important without the need for feature attribution (see Figure 4.3 left): signature is mostly dependent on µr, and', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 362, 'page_number': 23, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:57.056218+00:00', 'token_count': 57}),\n",
                            "  Document(page_content='Out of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly better than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 194, 'page_number': 11, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.890992+00:00', 'token_count': 63}),\n",
                            "  Document(page_content='log λ = 0 which is equivalent to V = µrλ, which is true by definition. It is, however, reassuring that we discover this relation without any prior knowledge.', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 390, 'page_number': 25, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:57.059859+00:00', 'token_count': 37}),\n",
                            "  Document(page_content='(2) Initialization scales. Each activation function is initialized to have ws = 1 and spline(x) ≈ 0 2. wb is initialized according to the Xavier initialization, which has been used to initialize linear layers in MLPs.', metadata={'parent_file_uuid': '36ed2f1a-57a5-489c-a4cb-fbdd25e2b038', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 77, 'page_number': 6, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:56.988853+00:00', 'token_count': 49}),\n",
                            "  Document(page_content='(since 𝑫 depends on the data 𝑥). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 448, 'page_number': 24, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.909104+00:00', 'token_count': 62}),\n",
                            "  Document(page_content='[19] Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. In: The International\\n\\nConference on Learning Representations (ICLR). 2024.', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 329, 'page_number': 18, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.900757+00:00', 'token_count': 41}),\n",
                            "  Document(page_content='with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to', metadata={'parent_file_uuid': '1a9d18a7-9499-47b6-abcc-4e82370028ee', 'creator_user_uuid': '5c37bf4c-002c-458d-9e68-03042f76a5b1', 'index': 25, 'page_number': 2, 'languages': ['eng'], 'link_texts': None, 'link_urls': None, 'links': None, 'created_datetime': '2024-06-28T07:23:52.879040+00:00', 'token_count': 51})],\n",
                            " 'route_name': 'search'}"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_core.runnables import (\n",
                "    Runnable,\n",
                "    RunnableLambda,\n",
                "    RunnablePassthrough,\n",
                "    chain,\n",
                ")\n",
                "from langchain.schema import StrOutputParser\n",
                "from operator import itemgetter\n",
                "from redbox.models import ChatRoute\n",
                "from redbox.models.chain import ChainInput\n",
                "\n",
                "from core_api.format import format_documents\n",
                "from core_api.runnables import make_chat_prompt_from_messages_runnable\n",
                "\n",
                "CONDENSE_SYSTEM_PROMPT = (\n",
                "    \"Given the following conversation and a follow up question, generate a follow \"\n",
                "    \"up question to be a standalone question. \"\n",
                "    \"You are only allowed to generate one question in response. \"\n",
                "    \"Include sources from the chat history in the standalone question created, \"\n",
                "    \"when they are available. \"\n",
                "    \"If you don't know the answer, just say that you don't know, \"\n",
                "    \"don't try to make up an answer. \\n\"\n",
                ")\n",
                "\n",
                "CONDENSE_QUESTION_PROMPT= \"{question}\\n=========\\n Standalone question: \"\n",
                "\n",
                "\n",
                "def build_condense_retrieval_chain(\n",
                "    llm,\n",
                "    retriever,\n",
                "    tokeniser,\n",
                "    env,\n",
                ") -> Runnable:\n",
                "    \n",
                "    def route(input_dict: dict):\n",
                "        if len(input_dict[\"chat_history\"]) > 0:\n",
                "            return RunnablePassthrough.assign(\n",
                "                question=make_chat_prompt_from_messages_runnable(\n",
                "                    system_prompt=env.ai.condense_system_prompt,\n",
                "                    question_prompt=env.ai.condense_question_prompt,\n",
                "                    input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
                "                    tokeniser=tokeniser,\n",
                "                )\n",
                "                | llm\n",
                "                | StrOutputParser()\n",
                "            )\n",
                "        else:\n",
                "            return RunnablePassthrough()\n",
                "\n",
                "    return (\n",
                "        RunnableLambda(route)\n",
                "        | RunnablePassthrough.assign(documents=retriever)\n",
                "        | RunnablePassthrough.assign(\n",
                "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
                "        )\n",
                "        | {\n",
                "            \"response\": make_chat_prompt_from_messages_runnable(\n",
                "                system_prompt=env.ai.retrieval_system_prompt,\n",
                "                question_prompt=env.ai.retrieval_question_prompt,\n",
                "                input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
                "                tokeniser=tokeniser,\n",
                "            )\n",
                "            | llm\n",
                "            | StrOutputParser(),\n",
                "            \"source_documents\": itemgetter(\"documents\"),\n",
                "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
                "        }\n",
                "    )\n",
                "\n",
                "# crag = make_chat_prompt_from_messages_runnable(\n",
                "#     system_prompt=CONDENSE_SYSTEM_PROMPT,\n",
                "#     question_prompt=CONDENSE_QUESTION_PROMPT,\n",
                "#     input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
                "#     tokeniser=tokeniser,\n",
                "# ) | llm\n",
                "\n",
                "crag = build_condense_retrieval_chain(llm, retriever, tokeniser, env)\n",
                "\n",
                "params = ChainInput(\n",
                "    question=\"Give the full citation.\",\n",
                "    file_uuids=[\n",
                "        \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
                "        \"1a9d18a7-9499-47b6-abcc-4e82370028ee\" # MAMBA paper\n",
                "    ],\n",
                "    user_uuid=\"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
                "    chat_history=[\n",
                "        {\"text\": \"What is the fastest attention that the authors are aware of?\", \"role\": \"user\"},\n",
                "        {\"text\": \"The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\", \"role\": \"ai\"},\n",
                "    ],\n",
                ")\n",
                "\n",
                "crag.invoke(params.model_dump())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "redbox-Vh_-Fb0j-py3.11",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
