{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create evaluation dataset for Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook**\n",
    "\n",
    "Set the version of the evaluation dataset you are creating **[HERE](#setversion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#overview)\n",
    "* [Set version of the evaluation dataset](#setversion)\n",
    "* [Select files for creating evaluation dataset](#files)\n",
    "* [Imports](#imports)\n",
    "* [Generate Evaluation Dataset](#ragas)\n",
    "* [Save Evaluation Dataset](#save)\n",
    "* [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important to version the evaluations we are doing, including the input data used to generate evaluation datasets.\n",
    "\n",
    "This notebook uses the files you select in combination with the RAGAS framework to generate synthetic data. Two different LLMs are used, one for the 'generator' and one for the 'critic'.\n",
    "\n",
    "Please be aware the generating synthetic data will incur LLM API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a troubleshooting section at the end of this notebook [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the version of the evaluation dataset you will be creating in this notebook in the cell below**  <a class=\"anchor\" id=\"setversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"0.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select files that you will use to generate versioned evaluation dataset   <a class=\"anchor\" id=\"files\"></a>\n",
    "\n",
    "Now copy all the files you want to use to generate **THIS VERSION** of the evaluation dataset into `notebooks/evaluation/data/{DATA_VERSION}/raw/`\n",
    "\n",
    "Also upload these files to shared Google Drive and the corresponding version number/location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports <a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import typing as t\n",
    "import json\n",
    "import jsonlines\n",
    "import pickle\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetically generate evaluation dataset <a class=\"anchor\" id=\"ragas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS generating a synthetic test set detailed [HERE](https://docs.ragas.io/en/stable/getstarted/testset_generation.html). Perhaps not as SOTA as DeepEval (validate!), but it creates `input` AND `expected_output` for us. \n",
    "\n",
    "So we are not generating input questions based on our chunking strategy, however, we are using the same files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 4 minutes for 4 docs. Consider Langchain `unstructured`\n",
    "loader = DirectoryLoader(V_RAW)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Langchain documents for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_docs_to_jsonl(documents: t.Iterable[Document], file_path: str) -> None:\n",
    "    with jsonlines.open(file_path, mode=\"w\") as writer:\n",
    "        for doc in documents:\n",
    "            writer.write(doc.dict())\n",
    "\n",
    "\n",
    "def load_docs_from_jsonl(file_path) -> t.Iterable[Document]:\n",
    "    documents = []\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for doc in reader:\n",
    "            documents.append(Document(**doc))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(documents, V_CHUNKS / \"documents.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # to match core-api\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o\") # cheaper model with similar performance\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.4, reasoning: 0.3, multi_context: 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save RAGAS generated testset <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{V_SYNTHETIC}/ragas_testset.pkl', 'wb') as f:\n",
    "    pickle.dump(testset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe into a DeepEval compatible CSV & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = testset.to_pandas()\n",
    "\n",
    "# Rename the columns\n",
    "new_column_names = {\n",
    "    'question': 'input',\n",
    "    'contexts': 'context',\n",
    "    'ground_truth': 'expected_output',\n",
    "    # Add more column names here\n",
    "}\n",
    "\n",
    "testset_df_renamed = testset_df.rename(columns=new_column_names)\n",
    "\n",
    "#  DeepEval dataset format requires an 'actual_output' column\n",
    "testset_df_renamed['actual_output'] = ''\n",
    "testset_df_renamed = testset_df_renamed.drop(['evolution_type', 'metadata', 'episode_done'], axis=1)\n",
    "\n",
    "# Convert all columns to string & drop NaN - otherwise DeepEval will throw an Pydantic validation error\n",
    "testset_df_renamed = testset_df_renamed.astype(str)\n",
    "testset_df_renamed = testset_df_renamed.dropna()\n",
    "\n",
    "# save as CSV\n",
    "testset_df_renamed.to_csv(f'{V_SYNTHETIC}/ragas_synthetic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) View top 5 rows of synthetically generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting <a class=\"anchor\" id=\"troubleshooting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain DirectoryLoader Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into a poppler path error and poppler is installed and can be access from your virtual environment (by running `pdfinfo -v`), then close notebook and restart the Jupyter server from the terminal where the path is correctly set (by running `code notebooks/evaluation/evaluation_dataset_generation.ipynb`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAGAS synthetically generated evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found some rows of synthetically generated evaluation data from using the RAGAS framework, includes some NaN and/or not str type, which results in an error for DeepEval metrics, as these data fail Pydantic validation.\n",
    "\n",
    "To avoid this, ensure you turn RAGAS synthetically generated evaluation data to type str and remove rows of data with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepEval framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, this notebook only loads the evaluation dataset into DeepEval from a CSV. There is a JSON import option that we are not using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
