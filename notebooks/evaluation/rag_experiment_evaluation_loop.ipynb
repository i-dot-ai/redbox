{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Experiment Loop\n",
    "\n",
    "This notebook reproduces the workflow in [rag_experiment_evaluation.ipynb](rag_experiment_evaluation.ipynb) through a series of functions. \n",
    "\n",
    "__This allows us to loop through a CSV of experiments in order to systematically change prompts and measure the results.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.models import Settings\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.file import UUID\n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ENV = Settings(minio_host=\"localhost\", elastic=ElasticLocalSettings(host=\"localhost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set evaluation data version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_VERSION = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set paths and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ENV.embedding_model\n",
    "INDEX = f\"{DATA_VERSION}-{MODEL}\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")\n",
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load embeddings into the index and get file UUIDs <a id=\"load-embeddings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES_CLIENT.indices.delete(index=INDEX, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_jsonl_to_index(file_path: Path, es_client: Elasticsearch, index: str) -> set:\n",
    "\n",
    "    file_uuids = set()\n",
    "\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for chunk_raw in reader:\n",
    "            chunk = json.loads(chunk_raw)\n",
    "            es_client.index(\n",
    "                index=index,\n",
    "                id=chunk[\"uuid\"],\n",
    "                body=chunk,\n",
    "            )\n",
    "\n",
    "            file_uuids.add(chunk[\"parent_file_uuid\"])\n",
    "\n",
    "    return file_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_UUIDS = load_chunks_from_jsonl_to_index(file_path=V_EMBEDDINGS / f\"{MODEL}.jsonl\", es_client=ES_CLIENT, index=INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define class for getting RAG outputs and evaluating based on prompts in experiments \n",
    "Note: these can be made more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.src import dependencies\n",
    "from redbox.models import ChatRoute, Settings\n",
    "from redbox.models.chain import ChainInput\n",
    "\n",
    "from typing import Annotated\n",
    "from fastapi import Depends\n",
    "from tiktoken import Encoding\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from core_api.src.format import format_documents\n",
    "from core_api.src.runnables import make_chat_prompt_from_messages_runnable\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from core_api.src.dependencies import get_tokeniser\n",
    "from core_api.src.retriever import ParameterisedElasticsearchRetriever\n",
    "\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")\n",
    "\n",
    "LLM = ChatLiteLLM(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "class Experiment:\n",
    "\n",
    "    '''Class for a evaluation experiment'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 experiment_name: str, \n",
    "                 retrieval_system_prompt: str, \n",
    "                 retrieval_question_prompt: str\n",
    "                 ):\n",
    "        \n",
    "        self.experiment_name = experiment_name\n",
    "        self.retrieval_system_prompt = retrieval_system_prompt\n",
    "        self.retrieval_question_prompt = retrieval_question_prompt\n",
    "        self.eval_results = None\n",
    "\n",
    "    def get_parameterised_retriever(self,\n",
    "                                    env: Annotated[Settings, Depends(ENV)],\n",
    "                                    es: Annotated[Elasticsearch, Depends(dependencies.get_elasticsearch_client)]\n",
    "        ) -> BaseRetriever:\n",
    "        \"\"\"Creates an Elasticsearch retriever runnable.\n",
    "\n",
    "        Runnable takes input of a dict keyed to question, file_uuids and user_uuid.\n",
    "\n",
    "        Runnable returns a list of Chunks.\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            \"size\": env.ai.rag_k,\n",
    "            \"num_candidates\": env.ai.rag_num_candidates,\n",
    "            \"match_boost\": 1,\n",
    "            \"knn_boost\": 1,\n",
    "            \"similarity_threshold\": 0,\n",
    "        }\n",
    "\n",
    "        return ParameterisedElasticsearchRetriever(\n",
    "            es_client=es,\n",
    "            index_name=INDEX,\n",
    "            params=default_params,\n",
    "            embedding_model=dependencies.get_embedding_model(env),\n",
    "        ).configurable_fields(\n",
    "            params=ConfigurableField(\n",
    "                id=\"params\", name=\"Retriever parameters\", description=\"A dictionary of parameters to use for the retriever.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def build_retrieval_chain(\n",
    "        self,\n",
    "        llm: Annotated[ChatLiteLLM, Depends(dependencies.get_llm)],\n",
    "        retriever: Annotated[VectorStoreRetriever, Depends(dependencies.get_parameterised_retriever)],\n",
    "        tokeniser: Annotated[Encoding, Depends(dependencies.get_tokeniser)],\n",
    "        env: Annotated[Settings, Depends(dependencies.get_env)]\n",
    "    ) -> Runnable:\n",
    "        return (\n",
    "            RunnablePassthrough.assign(documents=retriever)\n",
    "            | RunnablePassthrough.assign(\n",
    "                formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
    "            )\n",
    "            | {\n",
    "                \"response\": make_chat_prompt_from_messages_runnable(\n",
    "                    system_prompt=self.retrieval_system_prompt,\n",
    "                    question_prompt=self.retrieval_question_prompt,\n",
    "                    input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "                    tokeniser=tokeniser,\n",
    "                )\n",
    "                | llm\n",
    "                | StrOutputParser(),\n",
    "                \"source_documents\": itemgetter(\"documents\"),\n",
    "                \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def get_rag_results(self,\n",
    "                        question,\n",
    "                        ) -> dict:\n",
    "        \n",
    "        '''Get Redbox response for a given question.'''\n",
    "\n",
    "        retriever = self.get_parameterised_retriever(es=ES_CLIENT, env=ENV)\n",
    "\n",
    "        chain = self.build_retrieval_chain(llm=LLM,\n",
    "                                            retriever=retriever, \n",
    "                                            tokeniser=get_tokeniser(),\n",
    "                                            env=ENV)\n",
    "        \n",
    "        response = chain.invoke(\n",
    "            input=ChainInput(\n",
    "                question=question,\n",
    "                chat_history = [{\"text\": \"\", \"role\": \"user\"}],\n",
    "                file_uuids=list(FILE_UUIDS),\n",
    "                user_uuid=USER_UUID,\n",
    "            ).model_dump()\n",
    "        )\n",
    "\n",
    "        filtered_chunks = []\n",
    "\n",
    "        for chunk in response['source_documents']:\n",
    "\n",
    "            chunk = dict(chunk)\n",
    "            filtered_chunk = {'page_content': chunk['page_content'], 'page_number': \n",
    "                                chunk['metadata']['page_number'], \n",
    "                                'parent_file_uuid': chunk['metadata']['parent_file_uuid']}\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "        return {\"output_text\": response[\"response\"], \n",
    "                \"source_documents\": filtered_chunks}\n",
    "    \n",
    "    def write_rag_results(self) -> None:\n",
    "        \n",
    "        '''Format and write Redbox responses to evaluation dataset.'''\n",
    "     \n",
    "        df = pd.read_csv(f\"{V_SYNTHETIC}/ragas_synthetic_data.csv\")\n",
    "        inputs = df[\"input\"].tolist()\n",
    "\n",
    "        df_function = df.copy()\n",
    "\n",
    "        actual_output = []\n",
    "        retrieval_context = []\n",
    "\n",
    "        for question in inputs:\n",
    "            \n",
    "            data = self.get_rag_results(question=question)\n",
    "            actual_output.append(data[\"output_text\"])\n",
    "            retrieval_context.append(data['source_documents'])\n",
    "\n",
    "        df_function[\"actual_output\"] = actual_output\n",
    "        df_function[\"retrieval_context\"] = retrieval_context\n",
    "\n",
    "        df_function_clean = df_function.dropna()\n",
    "        df_function_clean.to_csv(f\"{V_SYNTHETIC}/{self.experiment_name}_complete_ragas_synthetic_data.csv\", index=False)\n",
    "\n",
    "    def do_evaluation(self) -> None:\n",
    "        \n",
    "        '''\n",
    "        Calculate evaluation metrics for a synthetic RAGAS dataset, aggregate results\n",
    "        and write as CSV.\n",
    "        '''\n",
    "\n",
    "        dataset = EvaluationDataset()\n",
    "        dataset.add_test_cases_from_csv_file(\n",
    "            file_path=f'{V_SYNTHETIC}/{self.experiment_name}_complete_ragas_synthetic_data.csv',\n",
    "            input_col_name=\"input\",\n",
    "            actual_output_col_name=\"actual_output\",\n",
    "            expected_output_col_name=\"expected_output\",\n",
    "            context_col_name=\"context\",\n",
    "            context_col_delimiter= \";\",\n",
    "            retrieval_context_col_name=\"retrieval_context\",\n",
    "            retrieval_context_col_delimiter= \";\"\n",
    "        )\n",
    "\n",
    "        # Instantiate retrieval metrics\n",
    "        contextual_precision = ContextualPrecisionMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        contextual_recall = ContextualRecallMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        contextual_relevancy = ContextualRelevancyMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        # Instantiate generation metrics\n",
    "        answer_relevancy = AnswerRelevancyMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        faithfulness = FaithfulnessMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        hallucination = HallucinationMetric(\n",
    "            threshold=0.5, # default is 0.5\n",
    "            model=\"gpt-4o\",\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        self.eval_results = evaluate(\n",
    "            test_cases=dataset,\n",
    "            metrics=[\n",
    "                contextual_precision,\n",
    "                contextual_recall,\n",
    "                contextual_relevancy,\n",
    "                answer_relevancy,\n",
    "                faithfulness,\n",
    "                hallucination\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def write_evaluation_results(self) -> None:\n",
    "\n",
    "        metric_type = {\n",
    "        \"metric_name\": [\"Contextual Precision\", \"Contextual Recall\", \"Contextual Relevancy\", \"Answer Relevancy\", \"Faithfulness\", \"Hallucination\"],\n",
    "        \"metric_type\": [\"retrieval\", \"retrieval\", \"retrieval\", \"generation\", \"generation\", \"generation\"]}\n",
    "\n",
    "        evaluation = (\n",
    "            pd.DataFrame.from_records(asdict(result) for result in self.eval_results)\n",
    "            .explode(\"metrics_metadata\")\n",
    "            .reset_index(drop=True)\n",
    "            .assign(\n",
    "                metric_name=lambda df: df.metrics_metadata.apply(getattr, args=[\"metric\"]),\n",
    "                score=lambda df: df.metrics_metadata.apply(getattr, args=[\"score\"]),\n",
    "                reason=lambda df: df.metrics_metadata.apply(getattr, args=[\"reason\"]),\n",
    "            )\n",
    "            .merge(pd.DataFrame(metric_type), on=\"metric_name\")\n",
    "            .drop(columns=[\"success\", \"metrics_metadata\"])\n",
    "        )\n",
    "\n",
    "        evaluation.to_csv(f\"{V_RESULTS}/{self.experiment_name}_val_results.csv\", index=False)\n",
    "        evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load CSV of experiments. See Google Drive folder 'experiment_parameters' for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = pd.read_csv('data/experiment_parameters/prompt_engineering_experiment_data_v3.csv')\n",
    "\n",
    "# # Filter by experiment name if you wish to only run certain experimental parameters\n",
    "experiment_parameters = experiment_parameters[(experiment_parameters.experiment_name == 'original_prompt') |\n",
    "                                              (experiment_parameters.experiment_name == 'unhelpful_prompt')]\n",
    "\n",
    "experiment_parameters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Loop through experiments and pass parameters to each function, returning the concantenated evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in experiment_parameters.iterrows():\n",
    "\n",
    "    expt = Experiment(experiment_name = row[\"experiment_name\"],\n",
    "                      retrieval_system_prompt = row[\"retrieval_system_prompt\"],\n",
    "                      retrieval_question_prompt = row[\"retrieval_question_prompt\"])\n",
    "    \n",
    "    expt.write_rag_results()\n",
    "    expt.do_evaluation()\n",
    "    expt.write_evaluation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Load and visualise results\n",
    "Note: there are some complexities that could require additional analysis, such as the uncertainty associated with each individual LLM judge score and the wide range of scores (0 to 1 for some metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "experiments = []\n",
    "\n",
    "# baseline = pd.read_csv(f\"{V_RESULTS}/baseline.csv\")\n",
    "# baseline['experiment_name'] = 'baseline'\n",
    "# experiments.append(baseline)\n",
    "\n",
    "# Comment out if you only want to view baseline statistics\n",
    "# Populate with experiment names\n",
    "experiment_names = ['original_prompt', 'unhelpful_prompt']\n",
    "for experiment_name in experiment_names:\n",
    "    experiment = pd.read_csv(f\"{V_RESULTS}/{experiment_name}_generation_eval_results.csv\")\n",
    "    experiment['experiment_name'] = experiment_name\n",
    "    experiments.append(experiment)\n",
    "\n",
    "experiments_df = pd.concat(experiments)\n",
    "\n",
    "def empirical_ci(df: pd.DataFrame\n",
    "                 ) -> pd.DataFrame:\n",
    "\n",
    "    '''Calculate confidence intervals for aggregated metrics.'''\n",
    "\n",
    "    df_grouped = (df\n",
    "                  .groupby([\"experiment_name\", \"metric_name\"])['score']\n",
    "                  .agg([\"mean\", 'sem', 'min', 'max', 'count'])\n",
    "                  .reset_index()\n",
    "                  )\n",
    "        \n",
    "    ci = stats.t.interval(confidence=0.95, \n",
    "                          df=df_grouped['count']-1,\n",
    "                          loc=df_grouped['mean'],\n",
    "                          scale=df_grouped['sem'])\n",
    "\n",
    "    df_grouped['ci_low'] = ci[0]\n",
    "    df_grouped['ci_high'] = ci[1] \n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "# Note that the confidence intervals in sns.barplot is calculated by bootstrapping. \n",
    "# See empirical_ci() above for empirical confidence interval calculation. \n",
    "sns.barplot(experiments_df, x=\"score\", y=\"metric_name\", hue='experiment_name', errorbar=('ci', 95))\n",
    "\n",
    "experiment_metrics = empirical_ci(experiments_df)\n",
    "experiment_metrics.to_csv(f\"{V_RESULTS}/eval_results_full.csv\")\n",
    "experiment_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
