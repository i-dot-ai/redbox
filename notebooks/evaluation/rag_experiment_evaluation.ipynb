{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Experimentation notebook for trying to improve Redbox `RAG chat`  <a class=\"anchor\" id=\"title\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
            "* [Overview](#overview)\n",
            "* [Metrics](#metrics)\n",
            "    - [Contextual Precisio]()\n",
            "    - [Contextual Recall]()\n",
            "    - [Contextual Relevancy]()\n",
            "    - [Fathfulness]()\n",
            "    - [Answer Relevancy]()\n",
            "    - [Hallucination]()\n",
            "    \n",
            "**CODE STARTS HERE**\n",
            "* [Set version of the evaluation dataset](#setversion)\n",
            "* [Imports](#imports)\n",
            "* [Run Redbox Locally](#run-redbox)\n",
            "\n",
            "**EXPERIMENTATION**\n",
            "* [Get files that correspond to the version of evaluation dataset](#files)\n",
            "* [Load Evaluation Dataset into test cases](#load-test-cases)\n",
            "* [Generate `actual_output` using RAG and evaluation dataset](#evaluate)\n",
            "    - [Retrieval Evaluation Metrics]()\n",
            "    - [Generation Evaluation Metrics]()\n",
            "* [Analyse evaluation results](#analysis)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Overview <a class=\"anchor\" id=\"overview\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "This notebook allows you to experiment with the retrieval and generation parts of Redbox RAG chat **WITHIN** the notebook, and get back evaluation metrics quickly. This allows you to test if the changes you make can improve the evaluation metrics, compared against the current/deployed RAG chat endpoint.\n",
            "\n",
            "************\n",
            "\n",
            "Redbox RAG chat is made up of many components that work together to give the final RAG pipeline. Each component can be optimised, to hopefully improve the over all performance of the RAG pipeline for Redbox tasks. In order to track if changes made are improving or degrading Redbox performance, we need to establish an evaluation framework. The overall RAG pipeline can be broken down into two main parts:\n",
            "\n",
            "1. Retrieval - searching and returning the most relevant documents to answer a user question\n",
            "2. Generation - the ouput of the LLM after considering the retrieved documents, any prompts provided and the user question\n",
            "\n",
            "This notebook tests both the retrieval and generation sides of the RAG pipeline using specific metrics for each, using the `DeepEval` framework.\n",
            "\n",
            "\n",
            "For consistency across the team, it is important to evaluate Redbox RAG chat on one stable, numbered version of these data.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "\n",
            "-------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Metrics <a class=\"anchor\" id=\"metrics\"></a>\n",
            "\n",
            "Retrieval metrics\n",
            "- Contextual Precision\n",
            "- Contextual Recall\n",
            "- Contextual Relevancy\n",
            "\n",
            "Generation metrics\n",
            "- Faithfulness\n",
            "- Answer Relevancy\n",
            "- Hallucination\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Contextual Precision\n",
            "\n",
            "The contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your `retrieval_context` that are relevant to the given `input` are ranked higher than irrelevant ones."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Contextual Recall\n",
            "\n",
            "The contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Contextual Relevancy\n",
            "\n",
            "The contextual relevancy metric measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your `retrieval_context` for a given `input`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Faithfulness\n",
            "\n",
            "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`\n",
            "- `retrieval_context`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Answer Relevancy\n",
            "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Hallucination\n",
            "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`\n",
            "- `retrieval_context`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Set the version of the evaluation dataset you are using to evalute Redbox in the cell below**   <a class=\"anchor\" id=\"setversion\"></a>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "DATA_VERSION = \"0.1.0\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "\n",
            "ROOT = Path.cwd().parents[1]\n",
            "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
            "\n",
            "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
            "V_RAW = V_ROOT / \"raw\"\n",
            "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
            "V_CHUNKS = V_ROOT / \"chunks\"\n",
            "V_RESULTS = V_ROOT / \"results\"\n",
            "\n",
            "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
            "V_RAW.mkdir(parents=True, exist_ok=True)\n",
            "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
            "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
            "V_RESULTS.mkdir(parents=True, exist_ok=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "To save on API costs, we only need to generate a particular version of the evaluation dataset once. If you are using a previously generaterated evalutation dataset, **please download it from shared team location (Google Drive).**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Imports <a id=\"imports\"></a>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Add autoreloatd\n",
            "%reload_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from jose import jwt\n",
            "from uuid import UUID\n",
            "import requests\n",
            "import json\n",
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from dotenv import find_dotenv, load_dotenv\n",
            "_ = load_dotenv(find_dotenv())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Start Redbox Running locally <a id=\"run-redbox\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Start docker runtime, either Docker Desktop. However, if you are using colima run the following terminal command\n",
            "```bash\n",
            "colima start --memory 8\n",
            "``` "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---------------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### First-time setup"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "First time users need to do the following"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "poetry install\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Ensure you .env file has OpenAI API key in and has the following settings:"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "# === Object Storage ===\n",
            "\n",
            "MINIO_HOST=minio\n",
            "MINIO_PORT=9000\n",
            "MINIO_ACCESS_KEY=minioadmin\n",
            "MINIO_SECRET_KEY=minioadmin\n",
            "AWS_ACCESS_KEY=minioadmin\n",
            "AWS_SECRET_KEY=minioadmin\n",
            "\n",
            "AWS_REGION=eu-west-2\n",
            "\n",
            "# minio or s3\n",
            "OBJECT_STORE=minio\n",
            "BUCKET_NAME=redbox-storage-dev\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Build redbox docker images (this takes several minutes)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "docker compose build\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Build containers"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "If changes are made to the app, e.g. changes pulled in from main, it may require rebuilding docker images\n",
            "\n",
            "```bash\n",
            "docker compose build --no-cache\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Run Redbox locally"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Every time you start Redbox for evaluation (no Django frontend required), please run the following command**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "make eval_backend\n",
            "````"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The above command will bring up everything you need for the backend (`core-api`, `worker`, `mino`, `elasticsearch` and `redis`), then create the MinIO bucket needed to store raw files"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "----------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# EXPERIMENTATION\n",
            "\n",
            "Use code below to experiment, in order to improve evaluation metrics or address a performance issue\n",
            "\n",
            "For setting an initial baseline with the existing Redbox Core API endpoint, please start [HERE](#baseline)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Generate `actual_output` using RAG chat function and evaluation dataset"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We first need to upload files that we are going to 'RAG with'"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Get files that correspond to the version of evaluation dataset  <a class=\"anchor\" id=\"files\"></a>\n",
            "\n",
            "Copy all the files that match your {DATA_VERSION} into `notebooks/evaluation/data/{DATA_VERSION}/raw/`. Find these files on the shared Google Drive and the corresponding version number/location\n",
            "\n",
            "**It is really important to use the same files that were used to genearte this particular version of the evaluation dataset. A mismatch between the two will result in inaccurate evaluatoin metrics**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Only if you haven't uploaded files already** uncomment and run cell below"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# url = 'http://127.0.0.1:5002/file/upload'\n",
            "\n",
            "# headers={\n",
            "#     'accept': 'application/json',\n",
            "#     \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "# }\n",
            "# for file in V_RAW.glob(\"*.*\"):\n",
            "#     files = {'file': open(file, 'rb')}\n",
            "#     upload_file_response = requests.post(url, headers=headers, files=files)\n",
            "\n",
            "#     #TODO: Add some login in the loop to deal with status codes != 200\n",
            "#     # if upload_file_response.status_code != 200:\n",
            "#     #     print(\"Failed to upload data:\", upload_file_response.status_code)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "List files uploaded to server & view JSON response"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "url = 'http://127.0.0.1:5002/file/'\n",
            "\n",
            "headers={\n",
            "    'accept': 'application/json',\n",
            "    \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "}\n",
            "\n",
            "file_list_response = requests.get(url, headers=headers)\n",
            "\n",
            "if file_list_response.status_code == 200:\n",
            "    # Parse JSON from the response\n",
            "    data = file_list_response.json()\n",
            "    \n",
            "    # Pretty-print the JSON data\n",
            "    pretty_json = json.dumps(data, indent=4)\n",
            "    print(pretty_json)\n",
            "else:\n",
            "    print(\"Failed to retrieve data:\", file_list_response.status_code)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Get file status"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from redbox.models import FileStatus\n",
            "\n",
            "def pretty_upload_status(file_uuid: UUID, bearer_token: str) -> str:\n",
            "    headers={\n",
            "        'accept': 'application/json',\n",
            "        \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "    }\n",
            "    status = FileStatus(**requests.get(f\"http://127.0.0.1:5002/file/{file_uuid}/status\", headers=headers).json())\n",
            "\n",
            "    status_title = status.processing_status.title()\n",
            "    n_chunks = 0\n",
            "    if status.chunk_statuses is not None:\n",
            "        n_chunks = len(status.chunk_statuses)\n",
            "\n",
            "    if status.processing_status == \"embedding\" and n_chunks > 0 and status.chunk_statuses is not None:\n",
            "        n_chunks_embedded = len([chunk for chunk in status.chunk_statuses if chunk.embedded])\n",
            "        return f\"{status_title} ({n_chunks_embedded / n_chunks:.0%})\"\n",
            "    else:\n",
            "        return status_title\n",
            "\n",
            "statuses = [pretty_upload_status(file[\"uuid\"], bearer_token) for file in file_list_response.json()]\n",
            "statuses"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Please ensure all emeddings have been completed before proceeding!**\n",
            "\n",
            "Keep calm and go for a tea break!"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "--------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Generate `actual_output` & `retrieval_context` using in-notebook `rag_chat()` function <a id=\"evaluate\"></a>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_csv(f'{V_SYNTHETIC}/ragas_synthetic_data.csv')\n",
            "inputs = df['input'].tolist()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Using a `rag_chat()` function"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We can conceptualise RAG as having four mechanisms we might tune:\n",
            "\n",
            "* Chunking\n",
            "* Embedding\n",
            "* Retriever\n",
            "* Prompts\n",
            "\n",
            "The below `rag_chat()` function replicates the internal logic of the RAG endpoint. By editing and using it here, you can quickly iterate and test the retriever and prompt mechanisms using your stable, versioned data, giving sharable, reproducible results.\n",
            "\n",
            "As long as `rag_chat()` takes a question (and history) and produces an answer, it's a testable process that could be used in Redbox. Everything within the function is yours to play with -- prompts, retriever, everything."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Set the experiment name that you want to associate with your experiment in the cell below** - this will save the output scores with this experiment in the file name"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "EXPERIMENT_NAME = \"original_prompt\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from redbox.models import Settings\n",
            "\n",
            "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
            "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
            "from langchain_community.chat_models import ChatLiteLLM\n",
            "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
            "from langchain.chains.llm import LLMChain\n",
            "from langchain.prompts.prompt import PromptTemplate\n",
            "from langchain.globals import set_verbose\n",
            "\n",
            "set_verbose(False)\n",
            "\n",
            "# Core variables\n",
            "\n",
            "ENV = Settings()\n",
            "ENV.elastic.host = \"localhost\"\n",
            "LLM = ChatLiteLLM(\n",
            "    model=\"gpt-3.5-turbo\",\n",
            "    streaming=True,\n",
            ")\n",
            "EMBEDDING_MODEL = SentenceTransformerEmbeddings(model_name=ENV.embedding_model)\n",
            "VECTOR_STORE = ElasticsearchStore(\n",
            "    es_connection=ENV.elasticsearch_client(),\n",
            "    index_name=\"redbox-data-chunk\",\n",
            "    embedding=EMBEDDING_MODEL,\n",
            "    strategy=ApproxRetrievalStrategy(hybrid=False),\n",
            "    vector_query_field=\"embedding\",\n",
            ")\n",
            "RETRIEVER = VECTOR_STORE.as_retriever(\n",
            "    search_kwargs={\n",
            "        \"filter\": {\n",
            "            \"term\": {\n",
            "                \"creator_user_uuid.keyword\": str(UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"))\n",
            "            }\n",
            "        }\n",
            "    }\n",
            ")\n",
            "\n",
            "# Prompts\n",
            "\n",
            "WITH_SOURCES_PROMPT = \"\"\"You are RedBox Copilot. An AI focused on helping UK Civil Servants, Political Advisors and\\\n",
            "Ministers triage and summarise information from a wide variety of sources. You are impartial and\\\n",
            "non-partisan. You are not a replacement for human judgement, but you can help humans\\\n",
            "make more informed decisions. If you are asked a question you cannot answer based on your following instructions, you\\\n",
            "should say so. Be concise and professional in your responses. Respond in markdown format.\n",
            "\n",
            "=== RULES ===\n",
            "\n",
            "All responses to Tasks **MUST** be translated into the user's preferred language.\\\n",
            "This is so that the user can understand your responses.\\\n",
            "Given the following extracted parts of a long document and \\\n",
            "a question, create a final answer with Sources at the end.  \\\n",
            "If you don't know the answer, just say that you don't know. Don't try to make \\\n",
            "up an answer.\n",
            "If a user asks for a particular format to be returned, such as bullet points, then please use that format. \\\n",
            "If a user asks for bullet points you MUST give bullet points. \\\n",
            "If the user asks for a specific number or range of bullet points you MUST give that number of bullet points. \\\n",
            "For example\n",
            "QUESTION: Please give me 6-8 bullet points on tigers\n",
            "FINAL ANSWER: - Tigers are orange. \\n- Tigers are big. \\n- Tigers are scary. \\n- Tigers are cool. \\n- Tigers are \\\n",
            "cats. -\\n Tigers are animals. \\\n",
            "\n",
            "If the number of bullet points a user asks for is not supported by the amount of information that you have, then \\\n",
            "say so, else give what the user asks for. \\\n",
            "\n",
            "At the end of your response add a \"Sources:\" section with the documents you used. \\\n",
            "DO NOT reference the source documents in your response. Only cite at the end. \\\n",
            "ONLY PUT CITED DOCUMENTS IN THE \"Sources:\" SECTION AND NO WHERE ELSE IN YOUR RESPONSE. \\\n",
            "IT IS CRUCIAL that citations only happens in the \"Sources:\" section. \\\n",
            "This format should be <DocX> where X is the document UUID being cited.  \\\n",
            "DO NOT INCLUDE ANY DOCUMENTS IN THE \"Sources:\" THAT YOU DID NOT USE IN YOUR RESPONSE. \\\n",
            "YOU MUST CITE USING THE <DocX> FORMAT. NO OTHER FORMAT WILL BE ACCEPTED.\n",
            "Example: \"Sources: <DocX> <DocY> <DocZ>\"\n",
            "\n",
            "Use **bold** to highlight the most question relevant parts in your response.\n",
            "If dealing dealing with lots of data return it in markdown table format.\n",
            "\n",
            "QUESTION: {question}\n",
            "=========\n",
            "{summaries}\n",
            "=========\n",
            "FINAL ANSWER:\"\"\"\n",
            "\n",
            "STUFF_DOCUMENT_PROMPT = \"<Doc{parent_doc_uuid}>{page_content}</Doc{parent_doc_uuid}>\"\n",
            "\n",
            "CONDENSE_QUESTION_PROMPT = \"\"\"Given the following conversation and a follow up question,\n",
            "rephrase the follow up question to be a standalone question, in its original\n",
            "language. include the follow up instructions in the standalone question.\n",
            "\n",
            "Chat History:\n",
            "{chat_history}\n",
            "Follow Up Input: {question}\n",
            "Standalone question:\"\"\"\n",
            "\n",
            "# RAG function\n",
            "\n",
            "def rag_chat(question: str, previous_history: list[tuple[str, str]] | None = None) -> str:\n",
            "    docs_with_sources_chain = load_qa_with_sources_chain(\n",
            "        LLM,\n",
            "        chain_type=\"stuff\",\n",
            "        prompt=PromptTemplate.from_template(WITH_SOURCES_PROMPT),\n",
            "        document_prompt=PromptTemplate.from_template(STUFF_DOCUMENT_PROMPT),\n",
            "        verbose=True,\n",
            "    )\n",
            "\n",
            "    condense_question_chain = LLMChain(\n",
            "        llm=LLM, \n",
            "        prompt=PromptTemplate.from_template(CONDENSE_QUESTION_PROMPT),\n",
            "        verbose=False\n",
            "    )\n",
            "\n",
            "    standalone_question = condense_question_chain(\n",
            "        {\"question\": question, \"chat_history\": previous_history}\n",
            "    )[\"text\"]\n",
            "\n",
            "    docs = RETRIEVER.get_relevant_documents(standalone_question)\n",
            "\n",
            "    result = docs_with_sources_chain(\n",
            "        {\n",
            "            \"question\": standalone_question,\n",
            "            \"input_documents\": docs,\n",
            "        },\n",
            "    )\n",
            "\n",
            "    source_documents = [\n",
            "        {\n",
            "            \"page_content\": langchain_document.page_content,\n",
            "            \"file_uuid\": langchain_document.metadata.get(\"parent_doc_uuid\"),\n",
            "            \"page_numbers\": langchain_document.metadata.get(\"page_numbers\"),\n",
            "        }\n",
            "        for langchain_document in result.get(\"input_documents\", [])\n",
            "    ]\n",
            "\n",
            "    return {\n",
            "        \"output_text\": result[\"output_text\"],\n",
            "        \"source_documents\": source_documents\n",
            "    }\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "%%capture\n",
            "\n",
            "df_function = df.copy()\n",
            "\n",
            "retrieval_context = []\n",
            "actual_output = []\n",
            "\n",
            "for question in inputs:\n",
            "    data = rag_chat(question=question, previous_history=None)\n",
            "\n",
            "    retrieval_context.append(data['source_documents'])\n",
            "    actual_output.append(data['output_text'])\n",
            "\n",
            "df_function['actual_output'] = actual_output\n",
            "df_function['retrieval_context'] = retrieval_context"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Confirm actual_output & retrieved_context added to the dataframe"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_function.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Remove rows containing NaN to prevent Pydantic validation errors"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_function_clean = df_function.dropna()\n",
            "df_function_clean.to_csv(f'{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv', index=False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "----"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Evaluation Dataset into test cases <a class=\"anchor\" id=\"load-test-cases\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Put the CSV file that you want to use for evaluation into `/notebooks/evaluation/data/synthetic_data/` directory"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Import test cases from CSV"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from deepeval.dataset import EvaluationDataset\n",
            "\n",
            "dataset = EvaluationDataset()\n",
            "dataset.add_test_cases_from_csv_file(\n",
            "    file_path=f'{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv', # function\n",
            "    input_col_name=\"input\",\n",
            "    actual_output_col_name=\"actual_output\",\n",
            "    expected_output_col_name=\"expected_output\",\n",
            "    context_col_name=\"context\",\n",
            "    context_col_delimiter= \";\",\n",
            "    retrieval_context_col_name=\"retrieval_context\",\n",
            "    retrieval_context_col_delimiter= \";\"\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Evaluate RAG pipeline <a id=\"evaluate\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "DeepEval imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from deepeval import evaluate\n",
            "from deepeval.metrics import (\n",
            "    ContextualPrecisionMetric,\n",
            "    ContextualRecallMetric,\n",
            "    ContextualRelevancyMetric,\n",
            "    AnswerRelevancyMetric,\n",
            "    FaithfulnessMetric,\n",
            "    HallucinationMetric,\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Instantiate retrieval and generation evaluation metrics"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate retrieval metrics\n",
            "contextual_precision = ContextualPrecisionMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "contextual_recall = ContextualRecallMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "contextual_relevancy = ContextualRelevancyMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate generation metrics\n",
            "answer_relevancy = AnswerRelevancyMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "faithfulness = FaithfulnessMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "hallucination = HallucinationMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### View test cases"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset.test_cases"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Retrieval Evaluation\n",
            "Separate retrieval and generation evaluation results, as retrieval evalation can take some time"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "retrieval_eval_results = evaluate(\n",
            "    test_cases=dataset,\n",
            "    metrics=[\n",
            "        contextual_precision,\n",
            "        contextual_recall,\n",
            "        contextual_relevancy,\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Save retrieval evaluation results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle \n",
            "with open(f'{V_RESULTS}/{EXPERIMENT_NAME}_retrieval_eval_results_v{DATA_VERSION}', 'wb') as f:\n",
            "    pickle.dump(retrieval_eval_results, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open(f'{V_RESULTS}/{EXPERIMENT_NAME}_retrieval_eval_results_v{DATA_VERSION}', 'rb') as f:\n",
            "    retrieval_eval_results = pickle.load(f)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Generation Evaluation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "generation_eval_results = evaluate(\n",
            "    test_cases=dataset,\n",
            "    metrics=[\n",
            "        answer_relevancy,\n",
            "        faithfulness,\n",
            "        hallucination\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Save generation evaluation results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle \n",
            "with open(f'{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results_v{DATA_VERSION}', 'wb') as f:\n",
            "    pickle.dump(generation_eval_results, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open(f'{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results_v{DATA_VERSION}', 'rb') as f:\n",
            "    generation_eval_results = pickle.load(f)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Analyse evaluation results <a id=\"analysis\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Retrieval Evaluation Results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from dataclasses import asdict\n",
            "import pandas as pd\n",
            "\n",
            "retrieval_eval_metrics = (\n",
            "    pd.DataFrame.from_records(\n",
            "        asdict(result) for result in retrieval_eval_results\n",
            "    )\n",
            "    .explode(\"metrics\")\n",
            "    .reset_index(drop=True)\n",
            "    .assign(\n",
            "        metric_name = lambda df: df.metrics.apply(getattr, args=[\"__name__\"]),\n",
            "        score = lambda df: df.metrics.apply(getattr, args=[\"score\"]),\n",
            "        reason = lambda df: df.metrics.apply(getattr, args=[\"reason\"])\n",
            "    )\n",
            "    .drop(columns=['success'])\n",
            ")\n",
            "\n",
            "retrieval_eval_metrics.to_csv(f'{V_RESULTS}/{EXPERIMENT_NAME}_retrieval_eval_results_v{DATA_VERSION}.csv', index=False)\n",
            "retrieval_eval_metrics.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Generation Evaluation Results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from dataclasses import asdict\n",
            "import pandas as pd\n",
            "\n",
            "generation_eval_metrics = (\n",
            "    pd.DataFrame.from_records(\n",
            "        asdict(result) for result in generation_eval_results\n",
            "    )\n",
            "    .explode(\"metrics\")\n",
            "    .reset_index(drop=True)\n",
            "    .assign(\n",
            "        metric_name = lambda df: df.metrics.apply(getattr, args=[\"__name__\"]),\n",
            "        score = lambda df: df.metrics.apply(getattr, args=[\"score\"]),\n",
            "        reason = lambda df: df.metrics.apply(getattr, args=[\"reason\"])\n",
            "    )\n",
            "    .drop(columns=['success'])\n",
            ")\n",
            "\n",
            "generation_eval_metrics.to_csv(f'{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results_v{DATA_VERSION}.csv', index=False)\n",
            "generation_eval_metrics.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Initial aggregated view"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "(\n",
            "    retrieval_eval_metrics\n",
            "    .groupby(\"metric_name\")\n",
            "    .mean(\"score\")\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "(\n",
            "    generation_eval_metrics\n",
            "    .groupby(\"metric_name\")\n",
            "    .mean(\"score\")\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**THIS IS THE END OF THE EXPERIMENT**\n",
            "\n",
            "Return to [HERE](#evaluate) to try a new experiment\n",
            "\n",
            "The section below is to establish a baseline with the current Core API endpoint only"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "redbox-MiicHf1r-py3.11",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.8"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
